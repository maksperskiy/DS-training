{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27280920",
   "metadata": {},
   "source": [
    "###### Данные представляют собой набор значений размеров ограничительных рамок фигуры человека и их раположение на изображении 320x240px\n",
    "<img src='data img.jpg' height='200px' width='140px' align='left'></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stunning-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becoming-personality",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1016/2498605884.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_to_word_sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_object(data, path):\n",
    "    file = open(path, 'wb')\n",
    "    pickle.dump(data, file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('x.txt', 'rb')\n",
    "x = pickle.load(data)\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('y.txt', 'rb')\n",
    "y = pickle.load(data)\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#отсеивание негодных вариантов\n",
    "to_del = [9,15,21,22,23,24,25,26,27,29,30,31,33,37,41,45,46,47,48,52,53,54,56,57,60,65,66,67,68,70,71,72,73,76,77,79,85,86]\n",
    "to_del.reverse()\n",
    "for el in to_del:\n",
    "    x.pop(el)\n",
    "    y.pop(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "#создание массива со значениями: отношение высоты и ширины, скорость пикселей в секунду\n",
    "\n",
    "x_all = []\n",
    "for video in x:\n",
    "    x_prev = 0\n",
    "    y_prev = 0\n",
    "    x_video = []\n",
    "    for frame in video:\n",
    "        x_frame = [frame[1]/frame[2] if frame[1] and frame[2] and frame[1]/frame[2]<=10 else 0, \n",
    "                   (((frame[3]-x_prev)**2+(frame[4]-y_prev)**2)**0.5) if frame[3] and frame[4] and (((frame[3]-x_prev)**2+(frame[4]-y_prev)**2)**0.5)<=120 else 0]\n",
    "        x_prev = frame[3]\n",
    "        y_prev = frame[4]\n",
    "        x_video.append(x_frame)\n",
    "    x_all.append(x_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Интерполяция значений отношений сторон бокса\n",
    "\n",
    "x_ratio = []\n",
    "countttt = -1\n",
    "for video in x_all:\n",
    "    frames = [el for el in range(1, 301)]\n",
    "    Ycoord = []\n",
    "    id_to_del = []\n",
    "    for frame in video:\n",
    "        Ycoord.append(frame[0])\n",
    "    for index, val in enumerate(Ycoord):\n",
    "        if val==0:\n",
    "            id_to_del.append(index)\n",
    "    id_to_del.reverse()\n",
    "    \n",
    "    countttt += 1\n",
    "    print(countttt)\n",
    "    \n",
    "#     plt.plot(range(1,301), Ycoord)\n",
    "#     plt.show()\n",
    "    \n",
    "    for el in id_to_del:\n",
    "        frames.pop(el)\n",
    "        Ycoord.pop(el)\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    Ycoord = np.interp(range(1,301),frames,Ycoord)\n",
    "#     plt.plot(range(1,301), Ycoord)\n",
    "#     plt.show()\n",
    "    \n",
    "    x_conv = np.array(range(1,301)).astype(float)\n",
    "    y_conv = np.array(Ycoord).astype(float)\n",
    "\n",
    "    w = np.hanning(50)\n",
    "    y_conv = np.convolve(w/w.sum(), y_conv, mode='same')\n",
    "    \n",
    "    #x_ratio.append(y_conv.tolist())\n",
    "    x_ratio.append(Ycoord.tolist())\n",
    "    \n",
    "    plt.plot(x_conv, Ycoord)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#анализ значений скорости\n",
    "\n",
    "#OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображение значений скорости\n",
    "x_speed = []\n",
    "countttt = -1\n",
    "for video in x_all:\n",
    "    frames = [el for el in range(1, 301)]\n",
    "    Ycoord = []\n",
    "    for frame in video:\n",
    "        Ycoord.append(frame[1])\n",
    "    x_speed.append(Ycoord)\n",
    "    \n",
    "    countttt += 1\n",
    "    print(countttt)\n",
    "    \n",
    "    plt.plot(range(1,301), Ycoord)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Окончательный отбор обучающих данных\n",
    "test_id = [19,24,30,44,45,46,47,48]\n",
    "test_id.reverse()\n",
    "for el in test_id:\n",
    "    x_ratio.append(x_ratio[el])\n",
    "    x_ratio.pop(el)\n",
    "    x_speed.append(x_speed[el])\n",
    "    x_speed.pop(el)\n",
    "    y.append(y[el])\n",
    "    y.pop(el)\n",
    "\n",
    "to_del = [5,6,7,8,9,16,17,18,20,21,22,23,25,26,27,28,29,31,32,33,35,36,37,38,39,40]\n",
    "to_del.reverse()\n",
    "for el in to_del:\n",
    "    x_ratio.pop(el)\n",
    "    x_speed.pop(el)\n",
    "    y.pop(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x1_to_csv = []\n",
    "for i in x_ratio:\n",
    "    x1_to_csv.append(i)\n",
    "for i in x_speed:\n",
    "    x1_to_csv.append(i)\n",
    "\n",
    "data_to_object(x1_to_csv, 'x1_to_csv.txt')\n",
    "x1_to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#нормирование значений\n",
    "\n",
    "\n",
    "def normalize_data(data):\n",
    "    result = []\n",
    "    for el in data:\n",
    "        line_max = max(el)\n",
    "        line_min = min(el)\n",
    "        line_result = []\n",
    "        for i in el:\n",
    "            line_result.append((i-line_min)/(line_max-line_min))\n",
    "        result.append(line_result)\n",
    "    return result\n",
    "\n",
    "x_ratio_norm = normalize_data(x_ratio)\n",
    "x_speed_norm = normalize_data(x_speed)\n",
    "#вывод значений\n",
    "x_ratio_norm, x_speed_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#конкатенация\n",
    "\n",
    "x_all = []\n",
    "for video in range(len(x_ratio_norm)):\n",
    "    x_video = []\n",
    "    for frame in range(300):\n",
    "        x_video.append([x_ratio_norm[video][frame], x_speed_norm[video][frame]])\n",
    "    x_all.append(x_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "for el, indx in enumerate(y):\n",
    "    print(el, indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-antibody",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(x_all)\n",
    "# x_to_csv = []\n",
    "# for i in x_all:\n",
    "#     video = []\n",
    "#     for j in i:\n",
    "#         video.append(j[0])\n",
    "#     x_to_csv.append(video)\n",
    "# for i in x_all:\n",
    "#     video = []\n",
    "#     for j in i:\n",
    "#         video.append(j[1])\n",
    "#     x_to_csv.append(video)\n",
    "\n",
    "# data_to_object(x_to_csv, 'x_to_csv.txt')\n",
    "\n",
    "\n",
    "# len(y)\n",
    "# y_to_csv = []\n",
    "# for i in y:\n",
    "#     y_to_csv.append(i)\n",
    "\n",
    "# data_to_object(x_to_csv, 'y_to_csv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('weights.txt', 'rb')\n",
    "weights = pickle.load(data)\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(600, 128))\n",
    "# model.add(LSTM(128, return_sequences=True))\n",
    "# model.add(LSTM(64))\n",
    "# model.add(Dense(1, activation='softmax'))\n",
    "# model.summary()\n",
    "# model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "# model.add(Dense(1))\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,  weights=weights[:3], return_sequences=True, input_shape=(300, 2), activation='tanh'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(32,  weights=weights[3:6], activation='tanh'))\n",
    "model.add(Dense(1,  weights=weights[6:], activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "#запись лучших начальных весов в файл\n",
    "\n",
    "# print (model.get_weights())\n",
    "# weights = model.get_weights()\n",
    "# data_to_object(weights, 'weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_all[:-8])\n",
    "x_test = np.asarray(x_all[-8:])\n",
    "y_train = np.asarray(y[:-8])\n",
    "y_test = np.asarray(y[-8:])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-nevada",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 8\n",
    "x_test[0].shape\n",
    "x_input = x_test[:num].reshape((num, 300, 2))\n",
    "res = model.predict(x_input, verbose=0)\n",
    "print(res, y_test)\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('test loss, test acc:', loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = []\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "x_train , x_test = np.asarray(x_all[:-8]), np.asarray(x_all[-8:])\n",
    "y_train , y_test = np.asarray(y[:-8]), np.asarray(y[-8:])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=60)\n",
    "pred_values = model.predict(x_test)\n",
    "\n",
    "print(pred_values, y_test)\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "acc_score.append(accuracy)\n",
    "\n",
    "#---------------------\n",
    "\n",
    "x_train , x_test = np.asarray(x_all[8:]), np.asarray(x_all[:8])\n",
    "y_train , y_test = np.asarray(y[8:]), np.asarray(y[:8])\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=60)\n",
    "pred_values = model.predict(x_test)\n",
    "\n",
    "print(pred_values, y_test)\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "acc_score.append(accuracy)\n",
    "\n",
    "#---------------------\n",
    "\n",
    "x_train , x_test = np.asarray([*x_all[-8:], *x_all[:7]]), np.asarray(x_all[7:-8])\n",
    "y_train , y_test = np.asarray([*y[-8:], *y[:7]]), np.asarray(y[7:-8])   \n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=60)\n",
    "pred_values = model.predict(x_test)\n",
    "\n",
    "print(pred_values, y_test)\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "acc_score.append(accuracy)\n",
    "     \n",
    "#--------------------------------------------------\n",
    "    \n",
    "avg_acc_score = sum(acc_score)/3\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
